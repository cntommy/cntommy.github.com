% Created 2022-11-02 三 15:26
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Jun Gao}
\date{\textit{[2022-11-02 三 15:12]}}
\title{tmp}
\hypersetup{
 pdfauthor={Jun Gao},
 pdftitle={tmp},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.2 (Org mode 9.5.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{survey}
\label{sec:orgfd070b6}
\href{//select/items/1\_SA76QZ4Z}{[1] I. Graßl, 《A Survey on Reinforcement Learning for Dialogue Systems》, 页 6.}
\href{//select/items/1\_27FLF3KC}{[1] V. Uc-Cetina, N. Navarro-Guerrero, A. Martin-Gonzalez, C. Weber和S. Wermter, 《Survey on reinforcement learning for language processing》, Artif Intell Rev, 6月 2022, doi: 10.1007/s10462-022-10205-5.}
\href{//select/items/1\_BD3M93QV}{[1] W.-C. Kwan, H. Wang, H. Wang和K.-F. Wong, 《A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-Oriented Dialogue Policy Learning》. arXiv, 2022年7月10日. 见于: 2022年10月19日. [在线]. 载于: http://arxiv.org/abs/2202.13675}

\section{2022顶会 强化学习 对话 相关文章}
\label{sec:orgc2a6c54}
[1] G. S. Ramachandran, K. Hashimoto和C. Xiong, 《[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue》, 收入 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 5月 2022, 页 92–102. doi: 10.18653/v1/2022.acl-long.8.]]
Q1 论文试图解决什么问题？
离线强化学习从人类对话中学习会有偏差和泛化性差的问题

Q2 这是否是一个新的问题？
不是

Q3 这篇文章要验证一个什么科学假设？
能够通过细粒度的奖励（来自人类对话的意图）和对话策略保护机制 来改善性能和样本效率

Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？
 Dataefficient off-policy policy evaluation for reinforcement learning

Q5 论文中提到的解决方案之关键是什么？
加入了因果的奖励

Q7 用于定量评估的数据集是什么？代码有没有开源？
multiwoz2.0 and convlab
没有开源

\href{//select/items/1\_7DZRRPBZ}{[1] S. Verma, J. Fu, S. Yang和S. Levine, 《CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning》, 收入 Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, United States, 7月 2022, 页 4471–4491. doi: 10.18653/v1/2022.naacl-main.332.} \href{20221020105357.org}{chai}
Q1 论文试图解决什么问题？
完全用静态数据集通过离线强化学习训练对话策略，加上预训练模型进行对话生成，实现售货对话系统

Q2 这是否是一个新的问题？
不是，2017年已经有相关工作，属于 CraigslistBargain task

Q3 这篇文章要验证一个什么科学假设？
验证离线强化学习也能够用于训练任务型对话系统

Q7 用于定量评估的数据集是什么？代码有没有开源？
没有开源代码

Q10 下一步呢？有什么工作可以继续深入？
模型架构不太容易迁移到其他领域

\href{//select/items/1\_3466QBVQ}{[1] P. Cai, H. Wan, F. Liu, M. Yu, H. Yu和S. Joshi, 《Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition》, 收入 Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, United States, 7月 2022, 页 4781–4796. doi: 10.18653/v1/2022.naacl-main.352.} \href{20221020132554.org}{info\textsubscript{acq}}
\href{//select/items/1\_IGZX58HZ}{[1] C. Tian, W. Yin和M.-F. Moens, 《Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System》, 收入 Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, United States, 7月 2022, 页 565–577. doi: 10.18653/v1/2022.findings-naacl.43.} \href{20221020133831.org}{anti-over}
  Q1 论文试图解决什么问题？
解决了对话策略模块用强化学习训练中，动作估值与实际不符导致训练不稳定的问题

Q2 这是否是一个新的问题？
不是

Q3 这篇文章要验证一个什么科学假设？
改善动作估值能提高收敛速度和性能

Q5 论文中提到的解决方案之关键是什么？
因为训练前期估值可信度低，在训练的前期降低最好动作的估值，随后逐渐提高最好动作估值的权重

Q7 用于定量评估的数据集是什么？代码有没有开源？
数据集:
movie-ticket booking (Li et al., 2016, 2017)
 restaurant reservation and taxi ordering (Li et al., 2018)

\href{//select/items/1\_4S782Q6M}{[1] Y. Zhao, H. Qin, W. Zhenyu, C. Zhu和S. Wang, 《A Versatile Adaptive Curriculum Learning Framework for Task-oriented Dialogue Policy Learning》, 收入 Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, United States, 7月 2022, 页 711–723. doi: 10.18653/v1/2022.findings-naacl.54.} \href{20221020135523.org}{versa-adap}
  Q1 论文试图解决什么问题？
解决在强化学习中用课程学习训练对话策略时，对对话任务难度评估不可靠，对话策略对对话过程敏感的问题

Q2 这是否是一个新的问题？
不是

Q3 这篇文章要验证一个什么科学假设？
可以构建一个使用学习经验自动评估对话难度，同时自动选择需要的对话来训练的架构，来提高学习效率

Q7 用于定量评估的数据集是什么？代码有没有开源？
 MovieTicket Booking, Restaurant Reservation, Taxi Ordering (Li et al., 2016, 2018)
 没开源

Q10 下一步呢？有什么工作可以继续深入？
可以在多领域上拓展，如multiwoz

\href{//select/items/1\_NYQZCULP}{[1] A. Ohashi和R. Higashinaka, 《Adaptive Natural Language Generation for Task-oriented Dialogue via Reinforcement Learning》, 收入 Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea, 10月 2022, 页 242–252. 见于: 2022年10月17日. [在线]. 载于: https://aclanthology.org/2022.coling-1.19}
\href{//select/items/1\_85XTRJ6T}{[1] C. Geishauser等, 《Dynamic Dialogue Policy for Continual Reinforcement Learning》, 收入 Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea, 10月 2022, 页 266–284. 见于: 2022年10月17日. [在线]. 载于: https://aclanthology.org/2022.coling-1.21} \href{20221020142352.org}{continue-rl}
  Q1 论文试图解决什么问题？
为了解决对话系统中对话策略，利用强化学习持续学习的问题

Q2 这是否是一个新的问题？
\href{//select/items/1\_GQ5SG5IV}{[1] Y. Jang, J. Lee和K.-E. Kim, 《GPT-CRITIC: OFFLINE REINFORCEMENT LEARNING FOR END-TO-END TASK-ORIENTED DIALOGUE SYS-》, 页 16, 2022.} \href{20221020143336.org}{gpt-critic}
    Q1 论文试图解决什么问题？
解决传统强化学习生成的对话质量差的问题，利用离线强化学习对PLM生成的对话进行引导

Q2 这是否是一个新的问题？
不是

Q7 用于定量评估的数据集是什么？代码有没有开源？
multiwoz2.0 and convlab
没有开源

\href{//select/items/1\_IUKACNSI}{[1] I.-J. Liu, X. Yuan, M.-A. Côté, P.-Y. Oudeyer和A. G. Schwing, 《Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language》. arXiv, 2022年7月3日. doi: 10.48550/arXiv.2205.06111.}
\end{document}