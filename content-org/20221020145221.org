:PROPERTIES:
:ID:       C0A0F8B1-4D17-4C6C-9BB9-CB6429C821BF
:END:
#+TITLE: caspi
#+AUTHOR: Jun Gao
#+DATE: [2022-10-20 四 14:52]
#+HUGO_BASE_DIR: ../
#+HUGO_SECTION: notes

#+DOWNLOADED: screenshot @ 2022-10-27 11:38:35
[[file:../images/20221027-113835_screenshot.png]]

Q1 论文试图解决什么问题？
离线强化学习从人类对话中学习会有偏差和泛化性差的问题

Q2 这是否是一个新的问题？
不是

Q3 这篇文章要验证一个什么科学假设？
能够通过细粒度的奖励（来自人类对话的意图）和对话策略保护机制 来改善性能和样本效率

Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？
 Dataefficient off-policy policy evaluation for reinforcement learning

Q5 论文中提到的解决方案之关键是什么？
加入了因果的奖励

Q6 论文中的实验是如何设计的？

Q7 用于定量评估的数据集是什么？代码有没有开源？
multiwoz2.0 and convlab
没有开源

Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？

Q9 这篇论文到底有什么贡献？

Q10 下一步呢？有什么工作可以继续深入？

离线强化学习和在线强化学习的样本效率？
只在multiwoz上验证？
复现了哪些论文的
